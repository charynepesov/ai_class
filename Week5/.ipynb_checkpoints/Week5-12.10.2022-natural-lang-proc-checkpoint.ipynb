{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2a9e349d-4d97-4669-8d05-feaa75670681",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bilgilerin toplanması, ...\n",
    "# text mining / text analysis\n",
    "# natural language processing -> human - human AND human - computer interactions\n",
    "# ultimate goal: natural human to computer speaking\n",
    "\n",
    "# character recognition\n",
    "# spell checking\n",
    "# text generation\n",
    "# SAT scores\n",
    "# human resources\n",
    "# call centers\n",
    "# sign language\n",
    "\n",
    "# speech recognition\n",
    "# speech 2 face\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bbeace67-69a9-42dc-9516-2c71535ea1dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy as sp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "25ef76ff-073d-415f-974b-f696786c1366",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cb2eea8d-c384-45aa-a24a-3e1a33896519",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\nepes\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d34b036d-0836-4e22-87da-d072cd47d865",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e9aff6d3-ae20-4004-8fc2-ca65a903482a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Welcome readers.', 'I hope you find it interesting.', 'Please do reply.']\n"
     ]
    }
   ],
   "source": [
    "text = 'Welcome readers. I hope you find it interesting. Please do reply.'\n",
    "print(sent_tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "25f82ab7-0163-4978-97ce-66a81b17e1d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['PierreVinken', ',', '59', 'years', 'old', ',', 'will', 'join', 'as', 'nonexecutive', 'director', 'on', 'Nov.', '29', '.']\n"
     ]
    }
   ],
   "source": [
    "text = nltk.word_tokenize('PierreVinken, 59 years old, will join as nonexecutive director on Nov. 29 .')\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1137e130-91e0-4b4f-b950-756b0a97dee9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Do',\n",
       " \"n't\",\n",
       " 'have',\n",
       " 'a',\n",
       " 'nice',\n",
       " 'day.',\n",
       " 'I',\n",
       " ',',\n",
       " 'hope',\n",
       " 'you',\n",
       " 'find',\n",
       " 'the',\n",
       " 'book',\n",
       " 'interesting']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "tokenizer = TreebankWordTokenizer()\n",
    "tokenizer.tokenize(\"Don't have a nice day. I, hope you find the book interesting\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "abccdc10-7bdd-47b4-bdac-aa4f20c32af4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\nepes\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "WPT = nltk.WordPunctTokenizer()\n",
    "nltk.download('stopwords')\n",
    "stop_word_list = nltk.corpus.stopwords.words('turkish')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15ae9b01-ffab-4496-b5ca-45c5286c08d8",
   "metadata": {},
   "source": [
    "## Stemmer and Lemmatizer\n",
    "    - stemmer -> kelimenin kökünü bulur, bir kitap verilir.\n",
    "        - porter\n",
    "        - lancaster\n",
    "        - regexp\n",
    "        - snowball\n",
    "    - lemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2b9b9f4f-b8c0-421c-9707-50cffa208a1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'happi'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "stemmerporter = PorterStemmer()\n",
    "stemmerporter.stem('happiness')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2afe3028-d8bf-40b2-8461-73ebcee28ff8",
   "metadata": {},
   "source": [
    "## LancasterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7dfed22f-b4bd-41ec-8583-4bc95b6e7cf4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'happy'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import LancasterStemmer\n",
    "stemmerlan = LancasterStemmer()\n",
    "stemmerlan.stem('happiness')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4af8aa2-7206-4d5a-ac9e-ed24c91618a3",
   "metadata": {},
   "source": [
    "### RegexpStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9e81a957-1cc6-4eda-a80e-0996cbae336f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "work\n",
      "happiness\n",
      "pair\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import RegexpStemmer\n",
    "stemmerregexp = RegexpStemmer('ing')\n",
    "print(stemmerregexp.stem('working'))\n",
    "print(stemmerregexp.stem('happiness'))\n",
    "print(stemmerregexp.stem('pairing'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16ac75fb-5c71-4480-8762-fa38e9097659",
   "metadata": {},
   "source": [
    "### Snowball Stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6ad57b04-04df-487a-8707-17ef29e42e98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('arabic', 'danish', 'dutch', 'english', 'finnish', 'french', 'german', 'hungarian', 'italian', 'norwegian', 'porter', 'portuguese', 'romanian', 'russian', 'spanish', 'swedish')\n",
      "comiend\n",
      "manag\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import SnowballStemmer\n",
    "print(SnowballStemmer.languages)\n",
    "spanishstemmer = SnowballStemmer('spanish')\n",
    "print(spanishstemmer.stem('comiende'))\n",
    "\n",
    "frenchstemmer = SnowballStemmer('french')\n",
    "print(frenchstemmer.stem('manager'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "92876c70-b8a5-4d25-bc30-3886c14363c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'okul'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from TurkishStemmer import TurkishStemmer\n",
    "stemmer = TurkishStemmer()\n",
    "stemmer.stem('okuldakilerden')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49ea6c05-54b7-4c4d-a7a6-ffe6c0d701f1",
   "metadata": {},
   "source": [
    "### Lemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8e08aa1a-6544-4b6d-8e3a-88d8a985e3e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "#nltk.download('wordnet')\n",
    "#nltk.download('omw-1.4')\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer_output = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f850190b-cdc3-497f-8a2e-2c898ffa0b87",
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mwordnet\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('wordnet')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/wordnet\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\nepes/nltk_data'\n    - 'C:\\\\Users\\\\nepes\\\\miniconda3\\\\envs\\\\yapay-zeka\\\\nltk_data'\n    - 'C:\\\\Users\\\\nepes\\\\miniconda3\\\\envs\\\\yapay-zeka\\\\share\\\\nltk_data'\n    - 'C:\\\\Users\\\\nepes\\\\miniconda3\\\\envs\\\\yapay-zeka\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\nepes\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[1;32m~\\miniconda3\\envs\\yapay-zeka\\lib\\site-packages\\nltk\\corpus\\util.py:84\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     83\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 84\u001b[0m     root \u001b[38;5;241m=\u001b[39m \u001b[43mnltk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msubdir\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mzip_name\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     85\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m:\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\yapay-zeka\\lib\\site-packages\\nltk\\data.py:583\u001b[0m, in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    582\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 583\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mwordnet\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('wordnet')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/wordnet.zip/wordnet/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\nepes/nltk_data'\n    - 'C:\\\\Users\\\\nepes\\\\miniconda3\\\\envs\\\\yapay-zeka\\\\nltk_data'\n    - 'C:\\\\Users\\\\nepes\\\\miniconda3\\\\envs\\\\yapay-zeka\\\\share\\\\nltk_data'\n    - 'C:\\\\Users\\\\nepes\\\\miniconda3\\\\envs\\\\yapay-zeka\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\nepes\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "Input \u001b[1;32mIn [16]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mlemmatizer_output\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlemmatize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mworking\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(lemmatizer_output\u001b[38;5;241m.\u001b[39mlemmatize(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mworking\u001b[39m\u001b[38;5;124m'\u001b[39m, pos\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mv\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(lemmatizer_output\u001b[38;5;241m.\u001b[39mlemmatize(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mworks\u001b[39m\u001b[38;5;124m'\u001b[39m))\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\yapay-zeka\\lib\\site-packages\\nltk\\stem\\wordnet.py:45\u001b[0m, in \u001b[0;36mWordNetLemmatizer.lemmatize\u001b[1;34m(self, word, pos)\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlemmatize\u001b[39m(\u001b[38;5;28mself\u001b[39m, word: \u001b[38;5;28mstr\u001b[39m, pos: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[0;32m     34\u001b[0m     \u001b[38;5;124;03m\"\"\"Lemmatize `word` using WordNet's built-in morphy function.\u001b[39;00m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;124;03m    Returns the input word unchanged if it cannot be found in WordNet.\u001b[39;00m\n\u001b[0;32m     36\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;124;03m    :return: The lemma of `word`, for the given `pos`.\u001b[39;00m\n\u001b[0;32m     44\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 45\u001b[0m     lemmas \u001b[38;5;241m=\u001b[39m \u001b[43mwn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_morphy\u001b[49m(word, pos)\n\u001b[0;32m     46\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mmin\u001b[39m(lemmas, key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m lemmas \u001b[38;5;28;01melse\u001b[39;00m word\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\yapay-zeka\\lib\\site-packages\\nltk\\corpus\\util.py:121\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__getattr__\u001b[1;34m(self, attr)\u001b[0m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attr \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__bases__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    119\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLazyCorpusLoader object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__bases__\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 121\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__load\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    122\u001b[0m \u001b[38;5;66;03m# This looks circular, but its not, since __load() changes our\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;66;03m# __class__ to something new:\u001b[39;00m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, attr)\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\yapay-zeka\\lib\\site-packages\\nltk\\corpus\\util.py:86\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     84\u001b[0m             root \u001b[38;5;241m=\u001b[39m nltk\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msubdir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mzip_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     85\u001b[0m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m:\n\u001b[1;32m---> 86\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m     88\u001b[0m \u001b[38;5;66;03m# Load the corpus.\u001b[39;00m\n\u001b[0;32m     89\u001b[0m corpus \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__reader_cls(root, \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__kwargs)\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\yapay-zeka\\lib\\site-packages\\nltk\\corpus\\util.py:81\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     79\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     80\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 81\u001b[0m         root \u001b[38;5;241m=\u001b[39m \u001b[43mnltk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msubdir\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__name\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     82\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     83\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\yapay-zeka\\lib\\site-packages\\nltk\\data.py:583\u001b[0m, in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    581\u001b[0m sep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m70\u001b[39m\n\u001b[0;32m    582\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 583\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mwordnet\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('wordnet')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/wordnet\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\nepes/nltk_data'\n    - 'C:\\\\Users\\\\nepes\\\\miniconda3\\\\envs\\\\yapay-zeka\\\\nltk_data'\n    - 'C:\\\\Users\\\\nepes\\\\miniconda3\\\\envs\\\\yapay-zeka\\\\share\\\\nltk_data'\n    - 'C:\\\\Users\\\\nepes\\\\miniconda3\\\\envs\\\\yapay-zeka\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\nepes\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "print(lemmatizer_output.lemmatize('working'))\n",
    "print(lemmatizer_output.lemmatize('working', pos='v'))\n",
    "print(lemmatizer_output.lemmatize('works'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ce03253-bc5d-43a8-9042-327cbe80e9d3",
   "metadata": {},
   "source": [
    "- difference between stemming and lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2325ad9-61b1-4656-8ee8-3a6b615e4bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "stemmer_output = PorterStemmer()\n",
    "print(stemmer_output.stem('happiness'))\n",
    "stemmer_output.stem('Mens')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "449e7a61-ff9a-429f-9020-fdf5852c6d54",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer_output = WordNetLemmatizer()\n",
    "lemmatizer_output.lemmatize('Mens')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "377165de-35e2-4552-aae3-d7723d16333f",
   "metadata": {},
   "source": [
    "### POS-Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0510812e-2ea0-480a-bdf7-f52382fe30d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "text = word_tokenize('Drive into NLTK: POS tagging and POS tagger')\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "341a8408-6e14-4ecb-92e8-c7587c7b326c",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8de8dd95-eb8b-4c0a-99d3-8a511e110f96",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.pos_tag(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b6b0c4f-8c19-4edd-b2af-9f0e6c2ac923",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "text = word_tokenize(\"It is a pleasant day today\")\n",
    "nltk.pos_tag(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f7577da-4381-46b0-93b2-51246bfc1db8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
